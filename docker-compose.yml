services:
  # Zookeeper for Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - bigdata-network

  # Kafka
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    networks:
      - bigdata-network

  # MinIO (S3-compatible storage)
  minio:
    image: minio/minio:latest
    hostname: minio
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin123
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    networks:
      - bigdata-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  # ClickHouse
  clickhouse:
    image: clickhouse/clickhouse-server:latest
    hostname: clickhouse
    container_name: clickhouse
    ports:
      - "8123:8123"  # HTTP interface
      - "9002:9000"  # Native protocol (mapped to 9002 to avoid conflict with MinIO)
    environment:
      CLICKHOUSE_DB: air_quality
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: "default123"
    volumes:
      - clickhouse-data:/var/lib/clickhouse
      - ./clickhouse/init.sql:/docker-entrypoint-initdb.d/init.sql
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    networks:
      - bigdata-network

  # Spark Batch Processing Job (for ML Training)
  # Pipeline: Kafka → Bronze → Silver → Gold (sequential)
  spark-batch:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-batch
    hostname: spark-batch
    depends_on:
      - kafka
      - minio
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - MINIO_ENDPOINT=http://minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin123
      - MINIO_BUCKET=air-quality-data
      - KAFKA_TOPIC=openaq-raw-measurements
      - PYSPARK_SUBMIT_ARGS=--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4 pyspark-shell
    volumes:
      - ./spark:/app/spark
      - ./aqi_calculator.py:/app/aqi_calculator.py
    networks:
      - bigdata-network
    # Don't auto-start - run manually for batch processing
    restart: "no"

networks:
  bigdata-network:
    driver: bridge

volumes:
  minio-data:
  clickhouse-data:

